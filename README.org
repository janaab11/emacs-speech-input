#+TITLE: Emacs Speech Input

#+HTML: <img alt="GitHub" src="https://img.shields.io/github/license/lepisma/emacs-speech-input?style=flat-square">

Set of packages for speech and voice inputs in Emacs. Use cases are explained
next with the packages they are available in:

** Dictation with Real-Time Editing
This allows you to input faster[fn::Needs empirical validation.] than simply
using voice with post-edits or just typing in. ~esi-dictate~ allows blended
workflows of voice and keyboard input powered via LLMs for real time edits.

To understand dictation in ~esi-dictate~, we need to understand two layers of
input. First is the /content layer/ that provides most of the content that gets
written. This happens via real-time voice transcription systems. Second is the
/edit layer/ that makes changes on what's written via the content layer which
could happen in the following ways:

1. /Trigger based voice edits/. Content layer continues to transcribe speech and
   emit at ~point~. When a key combination is pressed, next utterance---till end
   of utterance is automatically detected---is taken as an edit suggestion for
   the content. This is currently supported.
2. /Keyboard edits/. Similar to previous but the edit suggestions are taken from
   keyboard entries in parallel. Mostly helpful for correcting typoes for
   specialized words. This is under development.
3. /Automated voice edits/. Similar to first, but the key trigger is not
   needed. The system will automatically figure out which utterance is an edit
   suggestion and continue from there.

For using the triger based edit you need to put ~dg.py~ (interfaces with Deepgram
for transcription) in your ~PATH~ somewhere and set up the following:

#+begin_src emacs-lisp
(use-package esi-dictate
  :vc (:fetcher github :repo lepisma/emacs-speech-input)
  :custom
  (esi-dictate-dg-api-key "Deepgram API Key"
  (esi-dictate-llm-provider (make-llm-openai :key "OpenAI API Key" :chat-model "gpt-4o-mini"))
  :bind (:map esi-dictate-mode-map
              ("C-g" . esi-dictate-stop)
              ("C-SPC" . esi-dictate-start-command-mode))
  :config
  (setq llm-warn-on-nonfree nil))
#+end_src

Start dictation mode using ~esi-dictate-start~. Use ~esi-dictate-start-command-mode~
(see above bindings) to trigger edits, and ~esi-dictate-stop~ to exit dictation
mode.

As of now this uses non-free models for both transcriptions (Deepgram) and edits
(GPT4o-mini). This will change in the future.

** Flight-mode Recording
This is available as a dynamic module (~esi-core~) which needs a bit of clean up
before I document the usage.
